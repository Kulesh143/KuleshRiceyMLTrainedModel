# -*- coding: utf-8 -*-
"""KRiceyMLTrainedModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VU37izj8TGapHYuUjhVpnLECZb9B9YBO
"""

# # import numpy as np
# # import pandas as pd
# # import os
# # from google.colab import drive
# # from sklearn.model_selection import train_test_split
# # from tensorflow.keras.preprocessing.image import ImageDataGenerator
# # from tensorflow.keras import layers, models, optimizers, callbacks
# # import matplotlib.pyplot as plt
# # import tensorflow as tf
# # from google.colab import files

# # # Mount Google Drive
# # drive.mount('/content/drive')

# # # Data Directory Setup
# # data_dir = r"/content/drive/My Drive/PaddyLeafDeficiency/rice_plant_lacks_nutrients"
# # os.listdir(data_dir)

# # nutrient = ["Nitrogen(N)", "Potassium(K)", "Phosphorus(P)"]
# # nutrient_dict = {i: os.path.join(data_dir, i) for i in nutrient}

# # # Function to count images in directories
# # def count_image_dir(nutrient_dict):
# #     return {i: len(os.listdir(nutrient_dict[i])) for i in nutrient_dict}

# # count_dict = count_image_dir(nutrient_dict)

# # # Adjust DataFrame to include healthy images and dummy unhealthy samples
# # def create_df(nutrient_dict):
# #     file_list = []
# #     # Add healthy images
# #     for nutrient_name, nutrient_path in nutrient_dict.items():
# #         file_list += list(map(lambda x: [os.path.join(nutrient_path, x), "Healthy"], os.listdir(nutrient_path)))

# #     # Add dummy 'Unhealthy' samples (use the same paths for simplicity)
# #     for i in range(50):  # Let's assume we add 50 dummy samples
# #         file_list.append([file_list[i % len(file_list)][0], "Unhealthy"])  # Duplicating image paths

# #     return pd.DataFrame(file_list, columns=["filepaths", "labels"])

# # # Create DataFrame with healthy images and dummy unhealthy samples
# # tb_df = create_df(nutrient_dict)

# # # Display the DataFrame
# # print(tb_df.head())
# import numpy as np
# import pandas as pd
# import os
# import shutil  # Import shutil for file operations
# from google.colab import drive
# from sklearn.model_selection import train_test_split
# from tensorflow.keras.preprocessing.image import ImageDataGenerator
# from tensorflow.keras import layers, models, optimizers, callbacks
# import matplotlib.pyplot as plt
# import tensorflow as tf
# from google.colab import files

# # Mount Google Drive
# drive.mount('/content/drive')

# # Data Directory Setup
# data_dir = r"/content/drive/My Drive/PaddyLeafDeficiency/rice_plant_lacks_nutrients"
# os.listdir(data_dir)

# nutrient = ["Nitrogen(N)", "Potassium(K)", "Phosphorus(P)"]
# nutrient_dict = {i: os.path.join(data_dir, i) for i in nutrient}

# # Function to count images in directories
# def count_image_dir(nutrient_dict):
#     return {i: len(os.listdir(nutrient_dict[i])) for i in nutrient_dict}

# count_dict = count_image_dir(nutrient_dict)

# # Adjust DataFrame to include healthy images and dummy unhealthy samples
# def create_df(nutrient_dict):
#     file_list = []
#     # Add healthy images
#     for nutrient_name, nutrient_path in nutrient_dict.items():
#         file_list += list(map(lambda x: [os.path.join(nutrient_path, x), "Healthy"], os.listdir(nutrient_path)))

#     # Add dummy 'Unhealthy' samples (use the same paths for simplicity)
#     for i in range(50):  # Let's assume we add 50 dummy samples
#         file_list.append([file_list[i % len(file_list)][0], "Unhealthy"])  # Duplicating image paths

#     return pd.DataFrame(file_list, columns=["filepaths", "labels"])

# # Create DataFrame with healthy images and dummy unhealthy samples
# tb_df = create_df(nutrient_dict)

# # Create a new directory for Unhealthy samples
# unhealthy_dir = os.path.join(data_dir, 'Unhealthy')
# os.makedirs(unhealthy_dir, exist_ok=True)  # Create the directory if it doesn't exist

# # Copy dummy unhealthy samples to the new Unhealthy directory
# for i in range(50):  # Assuming we have added 50 dummy samples
#     # Get the path of the dummy "Unhealthy" image
#     dummy_image_path = tb_df['filepaths'][len(nutrient_dict) + i % len(nutrient_dict)].replace("Healthy", "Unhealthy")

#     # Copy the image to the Unhealthy directory
#     shutil.copy(tb_df['filepaths'][i % len(tb_df)], unhealthy_dir)

# # Display the DataFrame
# print(tb_df.head())
# import numpy as np
# import pandas as pd
# import os
# import shutil  # Import shutil for file operations
# from google.colab import drive
# from sklearn.model_selection import train_test_split
# from tensorflow.keras.preprocessing.image import ImageDataGenerator
# from tensorflow.keras import layers, models, optimizers, callbacks
# import matplotlib.pyplot as plt
# import tensorflow as tf
# from google.colab import files

# # Mount Google Drive
# drive.mount('/content/drive')

# # Data Directory Setup
# data_dir = r"/content/drive/My Drive/PaddyLeafDeficiency/rice_plant_lacks_nutrients"
# os.listdir(data_dir)

# nutrient = ["Nitrogen(N)", "Potassium(K)", "Phosphorus(P)"]
# nutrient_dict = {i: os.path.join(data_dir, i) for i in nutrient}

# # Function to count images in directories
# def count_image_dir(nutrient_dict):
#     return {i: len(os.listdir(nutrient_dict[i])) for i in nutrient_dict}

# count_dict = count_image_dir(nutrient_dict)

# # Adjust DataFrame to include healthy images and dummy unhealthy samples
# def create_df(nutrient_dict):
#     file_list = []
#     # Add healthy images
#     for nutrient_name, nutrient_path in nutrient_dict.items():
#         file_list += list(map(lambda x: [os.path.join(nutrient_path, x), "Healthy"], os.listdir(nutrient_path)))

#     # Add dummy 'Unhealthy' samples (use the same paths for simplicity)
#     for i in range(50):  # Let's assume we add 50 dummy samples
#         file_list.append([file_list[i % len(file_list)][0], "Unhealthy"])  # Duplicating image paths

#     return pd.DataFrame(file_list, columns=["filepaths", "labels"])

# # Create DataFrame with healthy images and dummy unhealthy samples
# com_df = create_df(nutrient_dict)

# # Create a new directory for Unhealthy samples if not already created
# unhealthy_dir = os.path.join(data_dir, 'Unhealthy')
# os.makedirs(unhealthy_dir, exist_ok=True)  # Create the directory if it doesn't exist

# # Copy dummy unhealthy samples to the new Unhealthy directory
# for i in range(50):  # Assuming we have added 50 dummy samples
#     # Get the path of the dummy "Unhealthy" image
#     dummy_image_path = com_df['filepaths'][len(nutrient_dict) + i % len(nutrient_dict)].replace("Healthy", "Unhealthy")

#     # Copy the image to the Unhealthy directory
#     shutil.copy(com_df['filepaths'][i % len(tb_df)], unhealthy_dir)

# # Load and verify the Unhealthy images from a different directory
# unhealthy_images_dir = r"/content/drive/My Drive/PaddyLeafDeficiency/unhealthy_rice_plants"
# unhealthy_files = os.listdir(unhealthy_images_dir)

# # Display the unhealthy images and their labels
# unhealthy_df = pd.DataFrame(unhealthy_files, columns=["filepaths"])
# unhealthy_df['labels'] = "Unhealthy"  # Label all as unhealthy

# # Display the DataFrame for unhealthy images
# print(unhealthy_df.head())

# # If you want to combine with the main DataFrame
# tb_df = pd.concat([com_df, unhealthy_df], ignore_index=True)
# print(tb_df.head())
import numpy as np
import pandas as pd
import os
import shutil  # Import shutil for file operations
from google.colab import drive
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models, optimizers, callbacks
import matplotlib.pyplot as plt
import tensorflow as tf
from google.colab import files

# Mount Google Drive
drive.mount('/content/drive')

# Data Directory Setup
data_dir = r"/content/drive/My Drive/PaddyLeafDeficiency/rice_plant_lacks_nutrients"
os.listdir(data_dir)

# Define nutrient categories (for healthy images)
nutrient = ["Nitrogen(N)", "Potassium(K)", "Phosphorus(P)"]
nutrient_dict = {i: os.path.join(data_dir, i) for i in nutrient}

# Function to count images in directories
def count_image_dir(nutrient_dict):
    return {i: len(os.listdir(nutrient_dict[i])) for i in nutrient_dict}

count_dict = count_image_dir(nutrient_dict)

# Adjust DataFrame to include healthy images from the nutrient_dict
def create_df(nutrient_dict):
    file_list = []
    # Add healthy images
    for nutrient_name, nutrient_path in nutrient_dict.items():
        file_list += list(map(lambda x: [os.path.join(nutrient_path, x), "Healthy"], os.listdir(nutrient_path)))

    return pd.DataFrame(file_list, columns=["filepaths", "labels"])

# Create DataFrame with healthy images
healthy_df = create_df(nutrient_dict)

# Load and verify the Unhealthy images from the 'unhealthy_rice_plants' directory
unhealthy_images_dir = r"/content/drive/My Drive/PaddyLeafDeficiency/Unhealthy/"
unhealthy_files = os.listdir(unhealthy_images_dir)

# Create a DataFrame for unhealthy images
unhealthy_df = pd.DataFrame(list(map(lambda x: [os.path.join(unhealthy_images_dir, x), "Unhealthy"], unhealthy_files)),
                            columns=["filepaths", "labels"])

# Combine healthy and unhealthy DataFrames
tb_df = pd.concat([healthy_df, unhealthy_df], ignore_index=True)

# Display the combined DataFrame
print(tb_df.head())

# Optional: Count the number of healthy and unhealthy samples
print("Sample counts by label:")
print(tb_df['labels'].value_counts())

tb_df.columns

tb_df.shape

tb_df

tb_df.dtypes

# Check the distribution of labels
print(tb_df['labels'].value_counts())

# Checking the total sum of the missing values in every column to see if they are there
missing_values_in_columns_sum = tb_df.isna().sum()

# Genearte the total percentage of missing values if they are there
total_no_of_rows = tb_df.shape[0]
missing_values_percentage = (missing_values_in_columns_sum / total_no_of_rows) * 100

# taking the data type of the columns in the dataset
datatype_column = tb_df.dtypes

# Making a DataFrame to show and display the results
missing_info_in_dataset = pd.DataFrame({
    'Data Type': datatype_column,
    'Missing Values': missing_values_in_columns_sum,
    'Percentage Missing (%)': missing_values_percentage
})

# Now here the missing values sum and percentage is displayed
print(missing_info_in_dataset)

import os
import cv2
import numpy as np
from PIL import Image

# Data Directory Setup
data_dir = r"/content/drive/My Drive/PaddyLeafDeficiency/rice_plant_lacks_nutrients"
nutrient = ["Nitrogen(N)", "Potassium(K)", "Phosphorus(P)"]
nutrient_dict = {i: os.path.join(data_dir, i) for i in nutrient}

# Path for the Unhealthy folder
unhealthy_directory = r"/content/drive/My Drive/PaddyLeafDeficiency/Unhealthy"

def is_corrupted(image_path):
    try:
        img = Image.open(image_path)
        img.verify()  # Check if image is corrupt
        return False
    except (IOError, SyntaxError):
        return True

def resize_image(image, size=(128, 128)):
    return cv2.resize(image, size)

def normalize_image(image):
    return image / 255.0  # Scale pixel values to [0, 1]

def clean_image_dataset(input_dir):
    seen_images = set()  # To track duplicates
    for filename in os.listdir(input_dir):
        if filename.endswith(('.jpg', '.jpeg', '.JPG')):
            image_path = os.path.join(input_dir, filename)

            # Step 1: Remove corrupted images
            if is_corrupted(image_path):
                print(f"Corrupted image: {filename}")
                continue

            # Step 2: Read the image
            image = cv2.imread(image_path)
            if image is None:
                print(f"Unable to read image: {filename}. Skipping.")
                continue

            # Step 3: Resize image
            image = resize_image(image)

            # Step 4: Normalize image
            image = normalize_image(image)

            # Step 5: Remove duplicates
            image_hash = hash(image.tobytes())
            if image_hash in seen_images:
                print(f"Duplicate image: {filename}. Skipping.")
                continue
            seen_images.add(image_hash)

            # Step 6: Create a new filename with the pattern 'cleaned_<original_filename>'
            new_filename = f"cleaned_{filename}"
            output_path = os.path.join(input_dir, new_filename)

            # Save the cleaned image, overwriting with the new pattern
            if not cv2.imwrite(output_path, (image * 255).astype(np.uint8)):
                print(f"Failed to save image: {new_filename}")
                continue

            # Optionally remove the original image to avoid duplicates if needed
            try:
                os.remove(image_path)
                print(f"Original image {filename} removed.")
            except Exception as e:
                print(f"Error removing image {filename}: {e}")

# Clean images for each nutrient category and save them back to the same folder
for nutrient_name, nutrient_path in nutrient_dict.items():
    clean_image_dataset(nutrient_path)

# Clean images in the Unhealthy directory and save them back to the same folder
clean_image_dataset(unhealthy_directory)

# Splitting data into train, validation, and test sets
train_images, test_images = train_test_split(tb_df, test_size=0.15, random_state=42)
train_images, val_images = train_test_split(train_images, test_size=0.176, random_state=42)

train_images, test_images

train_images, val_images
train_images.head()

# Data Augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.2,
    zoom_range=0.2,
    fill_mode='nearest'
)
train_datagen

# Preprocessing for validation and test
val_datagen = ImageDataGenerator(rescale=1./255.)

# Creating train, validation, and test data generators
train_data = train_datagen.flow_from_dataframe(
    dataframe=train_images, x_col="filepaths", y_col="labels",
    batch_size=8, target_size=(224, 224), class_mode="binary"
)

val_data = val_datagen.flow_from_dataframe(
    dataframe=val_images, x_col="filepaths", y_col="labels",
    batch_size=8, target_size=(224, 224), class_mode="binary"
)

test_data = val_datagen.flow_from_dataframe(
    dataframe=test_images, x_col="filepaths", y_col="labels",
    batch_size=8, target_size=(224, 224), class_mode="binary"
)

# Define Residual Block
def ResidualBlock(x, filters, kernel_size=(3, 3), strides=(1, 1), downsampling=False):
    shortcut = x
    if downsampling:
        x = layers.Conv2D(filters, kernel_size, strides=strides, padding='same')(x)
        shortcut = layers.Conv2D(filters, (1, 1), strides=strides)(shortcut)
    else:
        x = layers.Conv2D(filters, kernel_size, strides=strides, padding='same')(x)

    x = layers.BatchNormalization()(x)  # Before activation
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(filters, kernel_size, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, shortcut])
    return layers.Activation('relu')(x)

import matplotlib.pyplot as plt
import os
import random

# Define paths to the nutrient image directories
nutrient_dict = {
    'Nitrogen (N)': '/content/drive/My Drive/PaddyLeafDeficiency/rice_plant_lacks_nutrients/Nitrogen(N)',
    'Phosphorus (P)': '/content/drive/My Drive/PaddyLeafDeficiency/rice_plant_lacks_nutrients/Phosphorus(P)',
    'Potassium (K)': '/content/drive/My Drive/PaddyLeafDeficiency/rice_plant_lacks_nutrients/Potassium(K)'
}

# Function to load images from the specified directory
def load_images(nutrient_dir, num_images=5):
    images = os.listdir(nutrient_dir)
    selected_images = random.sample(images, min(num_images, len(images)))  # Randomly select a few images
    return [os.path.join(nutrient_dir, img) for img in selected_images]

# Set up the plot for displaying images
plt.figure(figsize=(15, 10))
num_images_per_nutrient = 3  # Set the number of images to display per nutrient

for i, (nutrient, path) in enumerate(nutrient_dict.items()):
    images = load_images(path, num_images=num_images_per_nutrient)
    for j, img_path in enumerate(images):
        img = plt.imread(img_path)
        plt.subplot(len(nutrient_dict), num_images_per_nutrient, i * num_images_per_nutrient + j + 1)
        plt.imshow(img)
        plt.axis('off')  # Turn off axis
        plt.title(nutrient)

plt.tight_layout()
plt.show()

# ResNet Model
def ResNetModel(input_shape):
    inputs = layers.Input(shape=input_shape)

    # Initial Convolutional Layer
    x = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)

    # Adding Residual Blocks
    filter_sizes = [64, 128, 256, 512]
    num_blocks = [2, 2, 2, 2]

    for i in range(len(filter_sizes)):
        for j in range(num_blocks[i]):
            downsampling = (i > 0 and j == 0)
            x = ResidualBlock(x, filters=filter_sizes[i], strides=(2, 2) if downsampling else (1, 1), downsampling=downsampling)

    # Average Pooling Layer and Flatten
    x = layers.GlobalAveragePooling2D()(x)

    # Dense Layer with Sigmoid for Binary Classification
    outputs = layers.Dense(1, activation='sigmoid')(x)

    # Creating the Model
    model = models.Model(inputs, outputs)
    return model

# Define model parameters
input_shape = (224, 224, 3)

# Build and compile model
model = ResNetModel(input_shape=input_shape)
optimizer = optimizers.SGD(learning_rate=0.001, momentum=0.9)  # Adjusted learning rate
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Define Callbacks
checkpoint = callbacks.ModelCheckpoint(
    filepath="ResNet18_model.keras",
    monitor='val_accuracy',  # Monitor validation accuracy
    verbose=1,
    save_best_only=True,
    mode='max'
)

reduce_lr = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.1,
    patience=4,
    verbose=1,
    min_lr=0.0001
)

early_stopping = callbacks.EarlyStopping(
    monitor='val_loss', patience=5, restore_best_weights=True  # Increased patience
)

tensorboard = callbacks.TensorBoard(log_dir='logs')

# Training the model
history = model.fit(
    train_data,
    steps_per_epoch=train_data.samples // train_data.batch_size,
    epochs=32,  # Increased epochs
    validation_data=val_data,
    validation_steps=val_data.samples // val_data.batch_size,
    callbacks=[checkpoint, reduce_lr, early_stopping, tensorboard]
)

# Plotting Training vs Validation Metrics
def plot_training_history(history):
    plt.figure(figsize=(12, 6))

    # Plot training and validation accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training vs. Validation Accuracy')
    plt.legend()
    plt.grid(True)

    # Plot training and validation loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training vs. Validation Loss')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

plot_training_history(history)

# Evaluation on Test Data
test_loss, test_acc = model.evaluate(test_data, steps=test_data.samples // test_data.batch_size)
print(f"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")

# Saving the final model
model.save("PaddyLeafDeficiency_ResNet18_Model.h5")

# Convert the model to TFLite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the model
with open("paddy_leaf_model.tflite", "wb") as f:
    f.write(tflite_model)

# Download the .tflite model
# files.download("paddy_leaf_model.tflite")

import os
import numpy as np
import tensorflow as tf
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load the saved model
model = tf.keras.models.load_model("PaddyLeafDeficiency_ResNet18_Model.h5")

# Evaluate the model on the test data
test_loss, test_acc = model.evaluate(test_data, steps=test_data.samples // test_data.batch_size)
print(f"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")

# Predict on the test dataset
test_data.reset()  # Reset the test data generator
predictions = model.predict(test_data, steps=test_data.samples // test_data.batch_size, verbose=1)

# Convert predictions to binary labels
predicted_labels = (predictions > 0.5).astype(int)

# Get true labels from the test dataset
true_labels = test_data.labels[:len(predicted_labels)]  # Adjust the length to match predictions

# Generate confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot confusion matrix
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Healthy', 'Unhealthy'], yticklabels=['Healthy', 'Unhealthy'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Generate classification report
report = classification_report(true_labels, predicted_labels, target_names=['Healthy', 'Unhealthy'])
print(report)

# Test all images in the Test_Images directory
def test_all_images(directory):
    for filename in os.listdir(directory):
        if filename.endswith('.jpg') or filename.endswith('.JPG'):  # Adjust as needed for other formats
            image_path = os.path.join(directory, filename)
            img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))
            img_array = tf.keras.preprocessing.image.img_to_array(img)
            img_array = np.expand_dims(img_array, axis=0)
            img_array /= 255.0  # Normalize the image

            prediction = model.predict(img_array)
            label = 'Healthy' if prediction < 0.5 else 'Unhealthy'
            print(f"Image: {filename}, Prediction: {label}")
             # Display the image along with the prediction
            plt.figure()
            plt.imshow(img)
            plt.title(f"Prediction: {label}")
            plt.axis('off')  # Turn off axis
            plt.show()

# Path to your test images directory
test_images_directory = "/content/drive/My Drive/PaddyLeafDeficiency/Test_Images/"  # Update with your actual test images directory

# Test on all images in the directory
test_all_images(test_images_directory)

# Split data into train, validation, and test sets
train_images, temp_images = train_test_split(tb_df, test_size=0.2, stratify=tb_df['labels'], random_state=42)
val_images, test_images = train_test_split(temp_images, test_size=0.5, stratify=temp_images['labels'], random_state=42)

# Data Augmentation and Preprocessing for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.2,
    zoom_range=0.2,
    fill_mode='nearest'
)

# Preprocessing for validation and test
val_datagen = ImageDataGenerator(rescale=1./255.)

# Creating train, validation, and test data generators
train_data = train_datagen.flow_from_dataframe(
    dataframe=train_images, x_col="filepaths", y_col="labels",
    batch_size=8, target_size=(224, 224), class_mode="binary"
)

val_data = val_datagen.flow_from_dataframe(
    dataframe=val_images, x_col="filepaths", y_col="labels",
    batch_size=8, target_size=(224, 224), class_mode="binary"
)

test_data = val_datagen.flow_from_dataframe(
    dataframe=test_images, x_col="filepaths", y_col="labels",
    batch_size=8, target_size=(224, 224), class_mode="binary"
)

# VGG16 Model with Pre-trained Weights
from tensorflow.keras.applications import VGG16

# Load the VGG16 model with pre-trained weights (ImageNet), excluding top layers
def VGG16Model(input_shape):
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)

    # Freeze the layers in the base model to prevent them from training
    base_model.trainable = False

    # Adding custom layers on top of the base model
    x = layers.Flatten()(base_model.output)
    x = layers.Dense(512, activation='relu')(x)  # Adding a fully connected layer
    x = layers.Dropout(0.5)(x)  # Regularization to prevent overfitting
    outputs = layers.Dense(1, activation='sigmoid')(x)  # Binary classification layer

    # Define the new model
    model = models.Model(inputs=base_model.input, outputs=outputs)
    return model

# Define model parameters
input_shape = (224, 224, 3)

# Build and compile the model
model = VGG16Model(input_shape=input_shape)
optimizer = optimizers.Adam(learning_rate=0.0001)  # Use Adam optimizer for faster convergence
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Define Callbacks
checkpoint = callbacks.ModelCheckpoint(
    filepath="VGG16_model.keras",
    monitor='val_accuracy',  # Monitor validation accuracy
    verbose=1,
    save_best_only=True,
    mode='max'
)

reduce_lr = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.1,
    patience=3,  # Reduce learning rate after 3 epochs of no improvement
    verbose=1,
    min_lr=0.001
)

early_stopping = callbacks.EarlyStopping(
    monitor='val_loss', patience=5, restore_best_weights=True
)

tensorboard = callbacks.TensorBoard(log_dir='logs')

# Training the model
history = model.fit(
    train_data,
    steps_per_epoch=train_data.samples // train_data.batch_size,
    epochs=1,  # Decrease epochs since VGG16 is pre-trained
    validation_data=val_data,
    validation_steps=val_data.samples // val_data.batch_size,
    callbacks=[checkpoint, reduce_lr, early_stopping, tensorboard]
)

import matplotlib.pyplot as plt

# Plotting Training vs Validation Metrics
def plot_training_history(history):
    plt.figure(figsize=(12, 6))

    # Check if 'accuracy' key exists in the history, some models may use 'acc' instead
    if 'accuracy' in history.history:
        accuracy_key = 'accuracy'
    else:
        accuracy_key = 'acc'

    # Plot training and validation accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history[accuracy_key], label='Training Accuracy')
    plt.plot(history.history['val_' + accuracy_key], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training vs. Validation Accuracy')
    plt.legend()
    plt.grid(True)

    # Plot training and validation loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training vs. Validation Loss')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

# Example usage (assuming history is generated from model.fit)
plot_training_history(history)

# Evaluation on Test Data
test_loss, test_acc = model.evaluate(test_data, steps=test_data.samples // test_data.batch_size)
print(f"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")

# Saving the final model
model.save("PaddyLeafDeficiency_VGG16_Model.h5")

# Convert the model to TFLite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the model
with open("paddy_leaf_model_vgg16.tflite", "wb") as f:
    f.write(tflite_model)

import numpy as np
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import load_model
import os
import matplotlib.pyplot as plt

# Load the trained VGG16 model
model = load_model('PaddyLeafDeficiency_VGG16_Model.h5')

# Directory containing test images
test_images_dir = "/content/drive/My Drive/PaddyLeafDeficiency/Test_Images"
test_files = os.listdir(test_images_dir)

# Define a function to preprocess and predict on a single image
def predict_image(img_path, model):
    # Load image with target size of 224x224 (VGG16 input size)
    img = image.load_img(img_path, target_size=(224, 224))

    # Convert image to array
    img_array = image.img_to_array(img)

    # Reshape the image array to add an extra dimension (batch size)
    img_array = np.expand_dims(img_array, axis=0)

    # Normalize the image array (as per training, rescale by 1./255)
    img_array /= 255.0

    # Predict using the model
    prediction = model.predict(img_array)

    # Since it's a binary classification, we use a threshold of 0.5
    if prediction[0] < 0.5:
        return 'Healthy'
    else:
        return 'Unhealthy'

# Test the model on all images in the Test_Images directory
for test_file in test_files:
    img_path = os.path.join(test_images_dir, test_file)

    # Predict the class of the image
    prediction = predict_image(img_path, model)

    # Display the image and prediction result
    img = image.load_img(img_path)
    plt.imshow(img)
    plt.title(f"Prediction: {prediction}")
    plt.axis('off')
    plt.show()

import tensorflow as tf
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Assuming you have test data prepared as test_data
# ResNet model and VGG16 model already trained and saved

# Load your pre-trained models (ResNet and VGG16)
resnet_model = tf.keras.models.load_model("PaddyLeafDeficiency_ResNet18_Model.h5")
vgg16_model = tf.keras.models.load_model("PaddyLeafDeficiency_VGG16_Model.h5")

# Get the true labels (y_true) from your test dataset
y_true = test_data.classes  # Assuming test_data has .classes for true labels

# Get model predictions (convert probabilities to class labels using argmax if multi-class)
resnet_preds = resnet_model.predict(test_data)
resnet_preds = resnet_preds.argmax(axis=1)  # If it's a multi-class problem, otherwise remove this line

vgg16_preds = vgg16_model.predict(test_data)
vgg16_preds = vgg16_preds.argmax(axis=1)  # If it's a multi-class problem, otherwise remove this line

# Calculate metrics for ResNet
resnet_accuracy = accuracy_score(y_true, resnet_preds)
resnet_precision = precision_score(y_true, resnet_preds, average='weighted')  # Adjust 'average' for binary classification
resnet_recall = recall_score(y_true, resnet_preds, average='weighted')
resnet_f1 = f1_score(y_true, resnet_preds, average='weighted')

# Calculate metrics for VGG16
vgg16_accuracy = accuracy_score(y_true, vgg16_preds)
vgg16_precision = precision_score(y_true, vgg16_preds, average='weighted')
vgg16_recall = recall_score(y_true, vgg16_preds, average='weighted')
vgg16_f1 = f1_score(y_true, vgg16_preds, average='weighted')

# Display results
print("ResNet Model Metrics:")
print(f"Accuracy: {resnet_accuracy:.4f}")
print(f"Precision: {resnet_precision:.4f}")
print(f"Recall: {resnet_recall:.4f}")
print(f"F1-Score: {resnet_f1:.4f}")

# Optionally, print confusion matrix
print("\nResNet Confusion Matrix:")
print(confusion_matrix(y_true, resnet_preds))

print("\nVGG16 Model Metrics:")
print(f"Accuracy: {vgg16_accuracy:.4f}")
print(f"Precision: {vgg16_precision:.4f}")
print(f"Recall: {vgg16_recall:.4f}")
print(f"F1-Score: {vgg16_f1:.4f}")


print("\nVGG16 Confusion Matrix:")
print(confusion_matrix(y_true, vgg16_preds))

import tensorflow as tf
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load your pre-trained VGG16 model
vgg16_model = tf.keras.models.load_model("PaddyLeafDeficiency_VGG16_Model.h5")

# Assuming you have the test data prepared and true labels (y_true)
y_true = test_data.classes  # Assuming test_data has .classes for true labels

# Get model predictions (convert probabilities to class labels using argmax if multi-class)
vgg16_preds = vgg16_model.predict(test_data)
vgg16_preds = vgg16_preds.argmax(axis=1)  # If multi-class, otherwise skip this line for binary classification

# Calculate metrics for VGG16
vgg16_accuracy = accuracy_score(y_true, vgg16_preds)
vgg16_precision = precision_score(y_true, vgg16_preds, average='binary')  # Use 'binary' for two classes
vgg16_recall = recall_score(y_true, vgg16_preds, average='binary')
vgg16_f1 = f1_score(y_true, vgg16_preds, average='binary')

# Display the metrics
print("\nVGG16 Model Metrics:")
print(f"Accuracy: {vgg16_accuracy:.4f}")
print(f"Precision: {vgg16_precision:.4f}")
print(f"Recall: {vgg16_recall:.4f}")
print(f"F1-Score: {vgg16_f1:.4f}")

# Compute and print the confusion matrix
cm = confusion_matrix(y_true, vgg16_preds)
print("\nVGG16 Confusion Matrix:")
print(cm)

# Define labels for the binary classification problem (Healthy and Unhealthy)
labels = ['Healthy', 'Unhealthy']

# Visualize the confusion matrix using seaborn heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('VGG16 Confusion Matrix (Healthy vs Unhealthy)')
plt.show()